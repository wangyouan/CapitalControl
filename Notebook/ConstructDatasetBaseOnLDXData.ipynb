{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-09T02:48:30.911111Z",
     "start_time": "2024-09-09T02:48:29.541274Z"
    }
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from Constant import Constants as const\n",
    "from OrganizeData.step02_merge_all_financial_data import sort_csmar_data"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:31:31.252040Z",
     "start_time": "2024-09-08T13:31:31.064262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ldx_df: DataFrame = pd.read_pickle(os.path.join(const.TEMP_PATH, '20240905_ldy_china_data.pkl')).rename(columns={'code': const.TICKER, 'year': const.YEAR})\n",
    "er_guarantee_df: DataFrame = pd.read_csv(os.path.join(r'D:\\Onedrive\\Projects\\CapitalControl\\data\\KarXiong', 'df4.csv'), usecols=['Symbol', 'Year', 'TotalLoan', 'NumGuarantee']).rename(columns={\"Symbol\": const.TICKER, 'Year': const.YEAR})\n"
   ],
   "id": "8b8c4490827af21a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:31:35.496451Z",
     "start_time": "2024-09-08T13:31:35.405168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in ['bsize', 'totalassets', 'firmage', 'fixedassets', 'listage']:\n",
    "    ldx_df[f'ln_{key}'] = np.log(ldx_df[key] + 2)\n",
    "    \n",
    "ldx_df.sort_values(by=[const.TICKER, const.YEAR], ascending=True, inplace=True)\n",
    "ldx_df['lagged_at'] = ldx_df.groupby(const.TICKER)['totalassets'].shift(1)\n",
    "ldx_df['sale_diff'] = ldx_df.groupby(const.TICKER)['sales'].diff(1)\n",
    "ldx_df['sale_growth'] = ldx_df['sale_diff'] / ldx_df['lagged_at']\n",
    "ldx_df['fix_at'] = ldx_df['fixedassets'] / ldx_df['lagged_at']\n",
    "ldx_df['salecost_at'] = ldx_df['cost'] / ldx_df['lagged_at']\n",
    "ldx_df['rev_at'] = ldx_df['revenue'] / ldx_df['lagged_at']\n",
    "\n",
    "ldx_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "for key in ['zscore', 'tbq3', 'mb', 'incometaxtate', 'tbq2', 'roa', 'growth', 'tbq1', 'lev', 'tbq4', 'sale_growth', 'fix_at', 'salecost_at', 'rev_at']:\n",
    "    ldx_df.loc[ldx_df[key].notnull(), key] = winsorize(ldx_df[key].dropna(), limits=(0.01, 0.01))"
   ],
   "id": "e502cd90e12788eb",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "455efac78a5e230e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:36:07.819109Z",
     "start_time": "2024-09-08T13:36:07.791349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_df: DataFrame = ldx_df.merge(er_guarantee_df, on=[const.TICKER, const.YEAR], how='left')\n",
    "reg_df.loc[:, 'Post2017'] = (reg_df[const.YEAR] >= 2017).astype(int)\n",
    "\n",
    "# Filter the DataFrame to include only the years 2014 to 2017\n",
    "df5_filtered = er_guarantee_df[(er_guarantee_df[const.YEAR] >= 2014) & (er_guarantee_df[const.YEAR] <= 2017)]\n",
    "\n",
    "# Create a new column \"er_foreign_gua\" and set it to 0 initially\n",
    "reg_df['er_foreign_gua'] = 0\n",
    "\n",
    "# Identify Symbols with at least one \"NumGuarantee\" greater than 0 between 2014 and 2017\n",
    "symbols_with_gua = df5_filtered[df5_filtered['NumGuarantee'] > 0][const.TICKER].unique()\n",
    "\n",
    "reg_df.loc[reg_df[const.TICKER].isin(symbols_with_gua), 'er_foreign_gua'] = 1\n",
    "reg_df['has_guarantee'] = 0\n",
    "reg_df.loc[reg_df['NumGuarantee'] > 0, 'has_guarantee'] = 1"
   ],
   "id": "305461f2abcf1ac0",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:36:36.393546Z",
     "start_time": "2024-09-08T13:36:36.349539Z"
    }
   },
   "cell_type": "code",
   "source": "reg_df.to_pickle(os.path.join(const.TEMP_PATH, '20240908_temp_base_reg_data.pkl'))",
   "id": "9e9fb0c1add0a4df",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Construct some financial data from CSMAR",
   "id": "3a692b66c344d3cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:57:22.229088Z",
     "start_time": "2024-09-08T12:57:19.038017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '财务指标文件.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('CSR_Finidx.csv') as csv_file:\n",
    "        finidx_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Stkcd', 'Accper', 'Outcap', 'Surplus', 'D610000', 'B150101', 'B140204', 'B140101', 'B120101', 'A100000', 'A200000']).rename(columns={'D610000': 'OCF', 'B150101': 'NetIncome', 'B140204': 'TXPD', 'B140101': 'EarningBI', 'B120101': 'OperatingRevenue', 'A100000': 'TotalAssets', 'A200000': 'TotalLiabilities'})\n",
    "        finidx_df: DataFrame = sort_csmar_data(finidx_df)"
   ],
   "id": "2f505bbd0e87fa79",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangy\\AppData\\Local\\Temp\\ipykernel_27200\\218993644.py:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gov_grant_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Stkcd', 'Accper', 'Item', 'Amount']).rename(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['RDInvest', 'RDSpendSumRatio', 'RDPersonRatio', 'RDInvestNetprofitRatio', 'RDInvestRatio', 'RDSpendSum', 'RDExpenses', 'RDPerson']",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 34\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m zipfile\u001B[38;5;241m.\u001B[39mZipFile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(const\u001B[38;5;241m.\u001B[39mCSMAR_PATH, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m财务指标.zip\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m zip_ref:\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m zip_ref\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBDT_FinIndex.csv\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m csv_file:\n\u001B[1;32m---> 34\u001B[0m         rd_spending_df: DataFrame \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(csv_file, usecols\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSymbol\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEndDate\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDPerson\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDPersonRatio\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDSpendSum\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDSpendSumRatio\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDExpenses\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDInvest\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDInvestRatio\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRDInvestNetprofitRatio\u001B[39m\u001B[38;5;124m'\u001B[39m], dtype\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEndDate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstr\u001B[39m\u001B[38;5;124m'\u001B[39m})\n\u001B[0;32m     35\u001B[0m         rd_spending_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEndDate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(rd_spending_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEndDate\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoerce\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     36\u001B[0m         rd_spending_df\u001B[38;5;241m.\u001B[39mdropna(subset\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEndDate\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSymbol\u001B[39m\u001B[38;5;124m'\u001B[39m], how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\Users\\wangy\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[1;32mD:\\Users\\wangy\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mD:\\Users\\wangy\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[1;32mD:\\Users\\wangy\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1895\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   1897\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1898\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mapping[engine](f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions)\n\u001B[0;32m   1899\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1900\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Users\\wangy\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001B[0m, in \u001B[0;36mCParserWrapper.__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morig_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39musecols_dtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mset\u001B[39m(usecols)\u001B[38;5;241m.\u001B[39missubset(\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morig_names\n\u001B[0;32m    139\u001B[0m ):\n\u001B[1;32m--> 140\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_usecols_names(usecols, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morig_names)\n\u001B[0;32m    142\u001B[0m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnames) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(usecols):  \u001B[38;5;66;03m# type: ignore[has-type]\u001B[39;00m\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Users\\wangy\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:979\u001B[0m, in \u001B[0;36mParserBase._validate_usecols_names\u001B[1;34m(self, usecols, names)\u001B[0m\n\u001B[0;32m    977\u001B[0m missing \u001B[38;5;241m=\u001B[39m [c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m usecols \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m names]\n\u001B[0;32m    978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 979\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    980\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsecols do not match columns, columns expected but not found: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    981\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmissing\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    982\u001B[0m     )\n\u001B[0;32m    984\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m usecols\n",
      "\u001B[1;31mValueError\u001B[0m: Usecols do not match columns, columns expected but not found: ['RDInvest', 'RDSpendSumRatio', 'RDPersonRatio', 'RDInvestNetprofitRatio', 'RDInvestRatio', 'RDSpendSum', 'RDExpenses', 'RDPerson']"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:00:06.100282Z",
     "start_time": "2024-09-08T13:00:03.993724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '研发投入情况表.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('PT_LCRDSPENDING.csv') as csv_file:\n",
    "        rd_spending_df: DataFrame = pd.read_csv(csv_file, usecols=['Symbol', 'EndDate', 'RDPerson', 'RDPersonRatio', 'RDSpendSum', 'RDSpendSumRatio', 'RDExpenses', 'RDInvest', 'RDInvestRatio', 'RDInvestNetprofitRatio'], dtype={'EndDate': 'str'})\n",
    "        rd_spending_df['EndDate'] = pd.to_datetime(rd_spending_df['EndDate'], format='%Y-%m-%d', errors='coerce')\n",
    "        rd_spending_df.dropna(subset=['EndDate', 'Symbol'], how='any', inplace=True)\n",
    "        rd_spending_df[const.TICKER] = rd_spending_df['Symbol'].astype(int)\n",
    "        rd_spending_df[const.YEAR] = rd_spending_df['EndDate'].dt.year\n",
    "        for key in ['RDPerson', 'RDPersonRatio', 'RDSpendSum', 'RDSpendSumRatio', 'RDExpenses', 'RDInvest', 'RDInvestRatio', 'RDInvestNetprofitRatio']:\n",
    "            rd_spending_df[key] = rd_spending_df[key].astype(np.float64)\n",
    "        rd_spend_df1: DataFrame = rd_spending_df.groupby([const.TICKER, const.YEAR])[['RDPerson', 'RDSpendSum', 'RDExpenses', 'RDInvest']].sum()\n",
    "        rd_spend_df2: DataFrame = rd_spending_df.groupby([const.TICKER, const.YEAR])[['RDPersonRatio', 'RDSpendSumRatio', 'RDInvestRatio', 'RDInvestNetprofitRatio']].mean()\n",
    "        rd_spend_df: DataFrame = rd_spend_df1.merge(rd_spend_df2, left_index=True, right_index=True, how='outer').reset_index(drop=False)\n",
    "        \n",
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '政府补助.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('PT_LCGovGrants.csv') as csv_file:\n",
    "        gov_grant_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Stkcd', 'Accper', 'Item', 'Amount']).rename(\n",
    "            columns={'Stkcd': const.TICKER})\n",
    "        gov_grant_df = gov_grant_df[gov_grant_df['Item'] == '合计']\n",
    "        gov_grant_df['Accper'] = pd.to_datetime(gov_grant_df['Accper'], format='%Y-%m-%d', errors='coerce')\n",
    "        gov_grant_df[const.YEAR] = gov_grant_df['Accper'].dt.year\n",
    "        gov_grant_df['GovGrantAmount'] = gov_grant_df['Amount'].astype(float)\n",
    "        gov_grant_df.dropna(subset=['GovGrantAmount'], inplace=True)\n",
    "        gov_grant_df = gov_grant_df[gov_grant_df['GovGrantAmount'] > 0]\n",
    "        gov_grant_df2: DataFrame = gov_grant_df.groupby([const.TICKER, const.YEAR])['GovGrantAmount'].sum().reset_index(drop=False)\n",
    "        gov_grant_df2['lnGovGrantAmount'] = gov_grant_df2['GovGrantAmount'].apply(np.log)"
   ],
   "id": "eae3f48699455a63",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangy\\AppData\\Local\\Temp\\ipykernel_27200\\2889229940.py:16: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gov_grant_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Stkcd', 'Accper', 'Item', 'Amount']).rename(\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:02:01.271308Z",
     "start_time": "2024-09-08T13:01:58.658218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '财务指标.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('BDT_FinIndex.csv') as csv_file:\n",
    "        finindex_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Symbol', 'EndDate', 'TotalAssets', 'TotalLiabilities', 'OperatingRevenue', 'IncomeTaxTate', 'TaxBearing', 'BankLoanRatio'])\n",
    "        finindex_df[const.TICKER] = finindex_df['Symbol'].astype(int)\n",
    "        finindex_df['EndDate'] = pd.to_datetime(finindex_df['EndDate'], format='%Y-%m-%d', errors='coerce')\n",
    "        finindex_df[const.YEAR] = finindex_df['EndDate'].dt.year\n",
    "        finindex_df.drop_duplicates(subset=[const.TICKER, const.YEAR], keep='last', inplace=True)\n",
    "        finindex_df.drop(['Symbol', 'EndDate'], axis=1, inplace=True)\n",
    "        \n",
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '融资约束—WW指数.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('BDT_FinConstWW.csv') as csv_file:\n",
    "        fcww_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Symbol', 'Enddate', 'WW', 'OperatingRevenueGrowth'])\n",
    "        fcww_df[const.TICKER] = fcww_df['Symbol'].astype(int)\n",
    "        fcww_df['Enddate'] = pd.to_datetime(fcww_df['Enddate'], format='%Y-%m-%d', errors='coerce')\n",
    "        fcww_df[const.YEAR] = fcww_df['Enddate'].dt.year\n",
    "        fcww_df.drop_duplicates(subset=[const.TICKER, const.YEAR], keep='last', inplace=True)\n",
    "        fcww_df.drop(['Symbol', 'Enddate'], axis=1, inplace=True)\n",
    "        \n",
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '融资约束—SA指数.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('BDT_FinConstSA.csv') as csv_file:\n",
    "        fcsa_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Symbol', 'Enddate', 'SA'])\n",
    "        fcsa_df[const.TICKER] = fcsa_df['Symbol'].astype(int)\n",
    "        fcsa_df['Enddate'] = pd.to_datetime(fcsa_df['Enddate'], format='%Y-%m-%d', errors='coerce')\n",
    "        fcsa_df[const.YEAR] = fcsa_df['Enddate'].dt.year\n",
    "        fcsa_df.drop_duplicates(subset=[const.TICKER, const.YEAR], keep='last', inplace=True)\n",
    "        fcsa_df.drop(['Symbol', 'Enddate'], axis=1, inplace=True)\n",
    "        \n",
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '融资约束—KZ指数.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('BDT_FinConstKZ.csv') as csv_file:\n",
    "        fckz_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Symbol', 'Enddate', 'KZ', 'TobinQ'])\n",
    "        fckz_df[const.TICKER] = fckz_df['Symbol'].astype(int)\n",
    "        fckz_df['Enddate'] = pd.to_datetime(fckz_df['Enddate'], format='%Y-%m-%d', errors='coerce')\n",
    "        fckz_df[const.YEAR] = fckz_df['Enddate'].dt.year\n",
    "        fckz_df.drop_duplicates(subset=[const.TICKER, const.YEAR], keep='last', inplace=True)\n",
    "        fckz_df.drop(['Symbol', 'Enddate'], axis=1, inplace=True)\n",
    "        \n",
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '融资约束—FC指数.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('BDT_FinConstFC.csv') as csv_file:\n",
    "        fcfc_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Symbol', 'Enddate', 'FC', 'EBIT'])\n",
    "        fcfc_df[const.TICKER] = fcfc_df['Symbol'].astype(int)\n",
    "        fcfc_df['Enddate'] = pd.to_datetime(fcfc_df['Enddate'], format='%Y-%m-%d', errors='coerce')\n",
    "        fcfc_df[const.YEAR] = fcfc_df['Enddate'].dt.year\n",
    "        fcfc_df.drop_duplicates(subset=[const.TICKER, const.YEAR], keep='last', inplace=True)\n",
    "        fcfc_df.drop(['Symbol', 'Enddate'], axis=1, inplace=True)\n"
   ],
   "id": "75b87d263185abe8",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T23:13:49.137848Z",
     "start_time": "2024-09-08T23:13:44.666051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '资产负债表.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('FS_Combas.csv') as csv_file:\n",
    "        combas_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Stkcd', 'Accper', 'A001212000', 'A001218000', 'A001219000']).rename(columns={'A001212000': 'FixedAssets', 'A001218000': 'Intangible', 'A001219000': 'RDSpend'})\n",
    "        combas_df: DataFrame = sort_csmar_data(combas_df)"
   ],
   "id": "d31e2a6678004ce6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with zipfile.ZipFile(os.path.join(const.CSMAR_PATH, '现金流分析.zip'), 'r') as zip_ref:\n",
    "    with zip_ref.open('FI_T6.csv') as csv_file:\n",
    "        t6_df: DataFrame = pd.read_csv(csv_file, on_bad_lines='skip', usecols=['Stkcd', 'Accper', 'F061201B']).rename(columns={'F061201B': 'DA'})\n",
    "        t6_df: DataFrame = sort_csmar_data(t6_df)"
   ],
   "id": "ff70310dcbff7e05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:29:58.731662Z",
     "start_time": "2024-09-08T13:29:58.589419Z"
    }
   },
   "cell_type": "code",
   "source": "csmar_data_df: DataFrame = finidx_df.merge(rd_spend_df, on=[const.TICKER, const.YEAR], how='outer').merge(gov_grant_df2, on=[const.TICKER, const.YEAR], how='outer').merge(finindex_df, on=[const.TICKER, const.YEAR], how='outer').merge(fcww_df, on=[const.TICKER, const.YEAR], how='outer').merge(fckz_df, on=[const.TICKER, const.YEAR], how='outer').merge(fcfc_df, on=[const.TICKER, const.YEAR], how='outer').merge(fcsa_df, on=[const.TICKER, const.YEAR], how='outer')",
   "id": "df88cecc658a5fbf",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T23:14:13.439921Z",
     "start_time": "2024-09-08T23:14:13.346755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csmar_data_df: DataFrame = pd.read_pickle(os.path.join(const.TEMP_PATH, '20240908_csmar_temp_data.pkl'))\n",
    "csmar_data_df = csmar_data_df.merge(t6_df, on=[const.TICKER, const.YEAR], how='outer').merge(combas_df, on=[const.TICKER, const.YEAR], how='outer')\n",
    "csmar_data_df.to_pickle(os.path.join(const.TEMP_PATH, '20240909_csmar_temp_data.pkl'))\n"
   ],
   "id": "fba0717bd6e3f0d2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:30:12.204046Z",
     "start_time": "2024-09-08T13:30:12.161723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drop_keys = list()\n",
    "for key in ['TotalAssets_x', 'TotalLiabilities_x', 'OperatingRevenue_x']:\n",
    "    drop_keys.append(key)\n",
    "    csmar_data_df.loc[:, key[:-2]] = csmar_data_df.loc[:, key].fillna(csmar_data_df[key.replace('_x', '_y')])\n",
    "    drop_keys.append(key.replace('_x', '_y'))\n",
    "    \n",
    "csmar_data_df.drop(drop_keys, axis=1, inplace=True)\n",
    "csmar_data_df.to_pickle(os.path.join(const.TEMP_PATH, '20240908_csmar_temp_data.pkl'))\n",
    "csmar_data_df = csmar_data_df[csmar_data_df[const.YEAR] > 2010].copy()\n",
    "csmar_data_df.shape"
   ],
   "id": "b1770cb1dc119130",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56771, 31)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:48:47.369258Z",
     "start_time": "2024-09-09T02:48:47.315403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csmar_data_df: DataFrame = pd.read_pickle(os.path.join(const.TEMP_PATH, '20240909_csmar_temp_data.pkl'))\n",
    "csmar_data_df = csmar_data_df[csmar_data_df[const.YEAR] > 2010].copy()\n",
    "csmar_data_df.keys()"
   ],
   "id": "eec99bbbb15863d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tic', 'EarningBI', 'TXPD', 'NetIncome', 'OCF', 'Surplus', 'Outcap',\n",
       "       'year', 'RDPerson', 'RDSpendSum', 'RDExpenses', 'RDInvest',\n",
       "       'RDPersonRatio', 'RDSpendSumRatio', 'RDInvestRatio',\n",
       "       'RDInvestNetprofitRatio', 'GovGrantAmount', 'lnGovGrantAmount',\n",
       "       'IncomeTaxTate', 'TaxBearing', 'BankLoanRatio',\n",
       "       'OperatingRevenueGrowth', 'WW', 'TobinQ', 'KZ', 'EBIT', 'FC', 'SA',\n",
       "       'TotalAssets', 'TotalLiabilities', 'OperatingRevenue', 'DA',\n",
       "       'FixedAssets', 'Intangible', 'RDSpend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:48:49.453367Z",
     "start_time": "2024-09-09T02:48:49.396229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csmar_data_df.sort_values(by=[const.TICKER, const.YEAR], ascending=True, inplace=True)\n",
    "csmar_data_df['lagged_at'] = csmar_data_df.groupby(const.TICKER)['TotalAssets'].shift(1)\n",
    "csmar_data_df['EarningBI_lat'] = csmar_data_df['EarningBI'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['NI_lat'] = csmar_data_df['NetIncome'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['CAPEX_lat'] = csmar_data_df['Outcap'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['RDExpenses_lat'] = csmar_data_df['RDExpenses'].fillna(0) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['RDSpendSum_lat'] = csmar_data_df['RDSpendSum'].fillna(0) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['RDSpend_lat'] = csmar_data_df['RDSpend'].fillna(0) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['RDInvest_lat'] = csmar_data_df['RDInvest'].fillna(0) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['GovGrantAmount_lat'] = csmar_data_df['GovGrantAmount'].fillna(0) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['LEV_lat'] = csmar_data_df['TotalLiabilities'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['EBIT_lat'] = csmar_data_df['EBIT'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['Int_lat'] = csmar_data_df['Intangible'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['OCF_lat'] = csmar_data_df['OCF'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['FA_lat'] = csmar_data_df['FixedAssets'] / csmar_data_df['lagged_at']\n",
    "csmar_data_df['CAPEX_RDEs_lat'] = (csmar_data_df['Outcap'] + csmar_data_df['RDExpenses'].fillna(0)) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['CAPEX_RDS_lat'] = (csmar_data_df['Outcap'] + csmar_data_df['RDSpend'].fillna(0)) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['CAPEX_RDI_lat'] = (csmar_data_df['Outcap'] + csmar_data_df['RDInvest'].fillna(0)) / csmar_data_df['lagged_at']\n",
    "csmar_data_df['FA_DA'] = csmar_data_df['FixedAssets'] + csmar_data_df['DA'].fillna(0)\n",
    "csmar_data_df['FA_DA_diff'] = csmar_data_df.groupby(const.TICKER)['FA_DA'].diff()\n",
    "csmar_data_df['DFIX'] = csmar_data_df['FA_DA_diff'] / csmar_data_df['lagged_at']\n",
    "\n",
    "csmar_data_df['DFIX2'] = csmar_data_df.groupby(const.TICKER)['FixedAssets'].pct_change(1)\n",
    "\n",
    "csmar_data_df['TXPD_Sales'] = csmar_data_df['TXPD'] / csmar_data_df['OperatingRevenue']\n"
   ],
   "id": "ef9c53a69cc62af4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8208\\1944682458.py:23: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  csmar_data_df['DFIX2'] = csmar_data_df.groupby(const.TICKER)['FixedAssets'].pct_change(1)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:48:53.647734Z",
     "start_time": "2024-09-09T02:48:53.610402Z"
    }
   },
   "cell_type": "code",
   "source": "csmar_data_df.replace([np.inf, -np.inf], np.nan, inplace=True)",
   "id": "9213eb3e27e5d2a5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:48:54.983834Z",
     "start_time": "2024-09-09T02:48:54.847619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dep_vars = ['RDPersonRatio', 'RDSpendSumRatio', 'RDInvestRatio', 'RDInvestNetprofitRatio', 'lnGovGrantAmount', 'IncomeTaxTate', 'TaxBearing', 'BankLoanRatio', 'EarningBI_lat', 'NI_lat', 'CAPEX_lat', 'RDExpenses_lat', 'RDSpend_lat', 'RDInvest_lat', 'GovGrantAmount_lat', 'LEV_lat', 'Int_lat', 'OCF_lat', 'FA_lat', 'CAPEX_RDEs_lat', 'CAPEX_RDS_lat', 'CAPEX_RDI_lat', 'FA_DA', 'RDSpendSum_lat', 'FA_DA_diff', 'DFIX', 'DFIX2', 'TXPD_Sales', 'EBIT_lat', const.TICKER, const.YEAR]\n",
    "win_vars = ['RDPersonRatio', 'RDSpendSumRatio', 'RDInvestRatio', 'RDInvestNetprofitRatio', 'IncomeTaxTate', 'TaxBearing', 'OperatingRevenueGrowth', 'WW', 'TobinQ', 'EarningBI_lat', 'NI_lat', 'CAPEX_lat', 'RDExpenses_lat', 'RDSpend_lat', 'RDInvest_lat', 'GovGrantAmount_lat', 'LEV_lat', 'Int_lat', 'OCF_lat', 'FA_lat', 'CAPEX_RDEs_lat', 'CAPEX_RDS_lat', 'CAPEX_RDI_lat', 'DFIX', 'DFIX2', 'TXPD_Sales', 'RDSpendSum_lat', 'EBIT_lat']\n",
    "\n",
    "for key in win_vars:\n",
    "    csmar_data_df.loc[csmar_data_df[key].notnull(), key] = winsorize(csmar_data_df[key].dropna(), limits=(0.01, 0.01))\n"
   ],
   "id": "9d369506ca6e3d00",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:48:58.692191Z",
     "start_time": "2024-09-09T02:48:58.592826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_df: DataFrame = pd.read_pickle(os.path.join(const.TEMP_PATH, '20240908_temp_base_reg_data.pkl'))\n",
    "reg_df2: DataFrame = reg_df.merge(csmar_data_df, how='left', on=[const.TICKER, const.YEAR])"
   ],
   "id": "e493d3d14386f547",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:49:00.153193Z",
     "start_time": "2024-09-09T02:48:59.811770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dep_df = csmar_data_df[dep_vars]\n",
    "\n",
    "for lead_year in range(1, 4):\n",
    "    dep_df[const.YEAR] -= 1\n",
    "    reg_df2: DataFrame = reg_df2.merge(dep_df, how='left', on=[const.TICKER, const.YEAR], suffixes=('', f'_{lead_year}'))\n",
    "    \n",
    "for year in range(2014, 2021):\n",
    "    reg_df2[f'dummy_{year}'] = reg_df2[const.YEAR].apply(lambda x: int(x == year))"
   ],
   "id": "88e3c17bfe945d46",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8208\\779950288.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dep_df[const.YEAR] -= 1\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8208\\779950288.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dep_df[const.YEAR] -= 1\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8208\\779950288.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dep_df[const.YEAR] -= 1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:49:02.916264Z",
     "start_time": "2024-09-09T02:49:02.910195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_df2['NumGuarantee'] = reg_df2['NumGuarantee'].fillna(0)\n",
    "reg_df2['TotalLoan'] = reg_df2['TotalLoan'].fillna(0)"
   ],
   "id": "5b028527dcd805d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:49:05.706361Z",
     "start_time": "2024-09-09T02:49:04.726586Z"
    }
   },
   "cell_type": "code",
   "source": "reg_df2.to_stata(os.path.join(const.OUTPUT_PATH, '20240909_cc_reg_data.dta'), write_index=False, version=119)",
   "id": "9f6859293b3efffb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T05:24:36.709629Z",
     "start_time": "2024-09-09T05:24:36.695304Z"
    }
   },
   "cell_type": "code",
   "source": "print(' '.join(dep_vars))",
   "id": "31fc63a1ed21a3d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDPersonRatio RDSpendSumRatio RDInvestRatio RDInvestNetprofitRatio lnGovGrantAmount IncomeTaxTate TaxBearing BankLoanRatio EarningBI_lat NI_lat CAPEX_lat RDExpenses_lat RDSpend_lat RDInvest_lat GovGrantAmount_lat LEV_lat Int_lat OCF_lat FA_lat CAPEX_RDEs_lat CAPEX_RDS_lat CAPEX_RDI_lat FA_DA RDSpendSum_lat FA_DA_diff DFIX DFIX2 TXPD_Sales EBIT_lat tic year\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
